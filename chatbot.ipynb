{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "7141e37055a9984976a2076320f2081b30ac4cb56c5f7539d2eec08ed0ff46c5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Chatbot, Encoder, Decoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "El archivo 'preguntas.txt' contiene 273 preguntas\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path('data')\n",
    "\n",
    "with open(data_dir/'preguntas.txt', 'r', encoding='utf-8') as f:\n",
    "    questions = f.readlines()\n",
    "\n",
    "with open(data_dir/'respuestas.txt', 'r', encoding='utf-8') as f:\n",
    "    answers = f.readlines()\n",
    "\n",
    "print(f\"El archivo 'preguntas.txt' contiene {len(questions)} preguntas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<START> \"rosebud\" <END>\n"
     ]
    }
   ],
   "source": [
    "START_TOKEN = '<START> '\n",
    "END_TOKEN = ' <END>'\n",
    "\n",
    "processed_questions = list(map(lambda x: unidecode(x.lower()), questions))\n",
    "processed_answers = list(map(lambda x: START_TOKEN + unidecode(x.lower().strip()) + END_TOKEN, answers))\n",
    "\n",
    "print(processed_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 1725 unique words\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(processed_questions + processed_answers)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "print(f\"There are {vocab_size} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoder input data shape: (273, 31)\nDecoder input data shape: (273, 31)\nDecoder target data shape: (273, 31, 1726)\n"
     ]
    }
   ],
   "source": [
    "# Preparing Encoder Input\n",
    "tokenized_questions = tokenizer.texts_to_sequences(processed_questions)\n",
    "max_input_length = max(list(map(len, tokenizer.texts_to_sequences(processed_questions))))\n",
    "encoder_input = np.array(pad_sequences(tokenized_questions, maxlen=max_input_length, padding='post'))\n",
    "print(f\"Encoder input data shape: {encoder_input.shape}\")\n",
    "\n",
    "# Preparing Decoder Input\n",
    "tokenized_answers = tokenizer.texts_to_sequences(processed_answers)\n",
    "max_output_length = max(list(map(len, tokenizer.texts_to_sequences(processed_answers))))\n",
    "decoder_input = np.array(pad_sequences(tokenized_answers, maxlen=max_input_length, padding='post'))\n",
    "print(f\"Decoder input data shape: {decoder_input.shape}\")\n",
    "\n",
    "# Preparing Target Output\n",
    "tokenized_output = list(map(lambda x: x[1:], tokenized_answers))\n",
    "padded_target = pad_sequences(tokenized_output, maxlen=max_input_length, padding='post')\n",
    "decoder_target = np.array(to_categorical(padded_target))\n",
    "print(f\"Decoder target data shape: {decoder_target.shape}\")\n",
    "\n",
    "with open(data_dir/'tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'tokenizer': tokenizer,\n",
    "        'vocab_size': vocab_size,\n",
    "        'max_input_length': max_input_length,\n",
    "        'max_output_length': max_output_length,\n",
    "        'model_bootstrap': (encoder_input[:1], decoder_input[:1], decoder_target[:1])   \n",
    "    }, f, protocol=pickle.DEFAULT_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "28/28 [==============================] - 15s 198ms/step - loss: 1.9370 - accuracy: 0.1519\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 6s 202ms/step - loss: 1.4041 - accuracy: 0.1822\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 6s 199ms/step - loss: 1.3133 - accuracy: 0.2297\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 6s 198ms/step - loss: 1.2644 - accuracy: 0.2486\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 6s 197ms/step - loss: 1.0438 - accuracy: 0.2968\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 5s 195ms/step - loss: 1.2076 - accuracy: 0.2716\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 5s 194ms/step - loss: 0.9944 - accuracy: 0.3114\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 6s 198ms/step - loss: 1.0604 - accuracy: 0.3073\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 5s 195ms/step - loss: 0.9691 - accuracy: 0.3250\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 6s 202ms/step - loss: 1.0103 - accuracy: 0.3429\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 6s 197ms/step - loss: 0.9692 - accuracy: 0.3654\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 6s 197ms/step - loss: 0.9091 - accuracy: 0.3685\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 5s 195ms/step - loss: 0.7610 - accuracy: 0.4024\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 5s 196ms/step - loss: 0.8586 - accuracy: 0.3963\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 5s 196ms/step - loss: 0.8828 - accuracy: 0.4143\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 6s 227ms/step - loss: 0.7199 - accuracy: 0.4563\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 7s 268ms/step - loss: 0.7066 - accuracy: 0.4599\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 8s 279ms/step - loss: 0.6035 - accuracy: 0.4887\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 9s 306ms/step - loss: 0.6308 - accuracy: 0.5001\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 9s 308ms/step - loss: 0.6605 - accuracy: 0.5114\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 9s 315ms/step - loss: 0.5847 - accuracy: 0.5552\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 7s 259ms/step - loss: 0.5377 - accuracy: 0.5771\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 6s 228ms/step - loss: 0.5296 - accuracy: 0.6035\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 6s 199ms/step - loss: 0.5141 - accuracy: 0.6578\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 5s 195ms/step - loss: 0.4333 - accuracy: 0.6734\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 5s 194ms/step - loss: 0.4333 - accuracy: 0.7227\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 5s 195ms/step - loss: 0.4074 - accuracy: 0.7080\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 6s 199ms/step - loss: 0.3810 - accuracy: 0.7539\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 7s 235ms/step - loss: 0.3729 - accuracy: 0.7650\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 6s 226ms/step - loss: 0.3309 - accuracy: 0.7864\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 7s 248ms/step - loss: 0.2954 - accuracy: 0.7998\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 6s 224ms/step - loss: 0.3027 - accuracy: 0.8031\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 6s 206ms/step - loss: 0.2767 - accuracy: 0.8492\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 6s 202ms/step - loss: 0.2509 - accuracy: 0.8440\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 6s 200ms/step - loss: 0.2535 - accuracy: 0.8618\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 6s 196ms/step - loss: 0.2259 - accuracy: 0.8471\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 6s 201ms/step - loss: 0.2047 - accuracy: 0.8642\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - 6s 199ms/step - loss: 0.1970 - accuracy: 0.8865\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - 5s 196ms/step - loss: 0.2022 - accuracy: 0.8933\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - 6s 198ms/step - loss: 0.1767 - accuracy: 0.8975\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - 6s 200ms/step - loss: 0.1657 - accuracy: 0.9195\n",
      "Epoch 42/100\n",
      "28/28 [==============================] - 6s 231ms/step - loss: 0.1508 - accuracy: 0.9099\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - 6s 202ms/step - loss: 0.1472 - accuracy: 0.9362\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - 6s 198ms/step - loss: 0.1417 - accuracy: 0.9333\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - 6s 200ms/step - loss: 0.1302 - accuracy: 0.9364\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - 5s 195ms/step - loss: 0.1220 - accuracy: 0.9476\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - 6s 200ms/step - loss: 0.1158 - accuracy: 0.9482\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - 6s 198ms/step - loss: 0.1103 - accuracy: 0.9576\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - 5s 195ms/step - loss: 0.1023 - accuracy: 0.9569\n",
      "Epoch 50/100\n",
      "28/28 [==============================] - 5s 195ms/step - loss: 0.0930 - accuracy: 0.9719\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - 5s 194ms/step - loss: 0.0944 - accuracy: 0.9708\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - 6s 208ms/step - loss: 0.0829 - accuracy: 0.9771\n",
      "Epoch 53/100\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 0.0779 - accuracy: 0.9812\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - 6s 199ms/step - loss: 0.0735 - accuracy: 0.9819\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - 6s 209ms/step - loss: 0.0739 - accuracy: 0.9869\n",
      "Epoch 56/100\n",
      "28/28 [==============================] - 5s 196ms/step - loss: 0.0682 - accuracy: 0.9894\n",
      "Epoch 57/100\n",
      "28/28 [==============================] - 5s 193ms/step - loss: 0.0612 - accuracy: 0.9942\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - 5s 193ms/step - loss: 0.0589 - accuracy: 0.9924\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - 5s 192ms/step - loss: 0.0568 - accuracy: 0.9905\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - 5s 192ms/step - loss: 0.0511 - accuracy: 0.9935\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - 5s 194ms/step - loss: 0.0494 - accuracy: 0.9959\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - 6s 203ms/step - loss: 0.0473 - accuracy: 0.9966\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - 6s 228ms/step - loss: 0.0449 - accuracy: 0.9960\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - 5s 196ms/step - loss: 0.0405 - accuracy: 0.9972\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - 5s 194ms/step - loss: 0.0376 - accuracy: 0.9972\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - 5s 196ms/step - loss: 0.0338 - accuracy: 0.9972\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - 6s 197ms/step - loss: 0.0333 - accuracy: 0.9981\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - 5s 196ms/step - loss: 0.0318 - accuracy: 0.9985\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - 6s 201ms/step - loss: 0.0314 - accuracy: 0.9989\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - 6s 196ms/step - loss: 0.0278 - accuracy: 0.9992\n",
      "Epoch 71/100\n",
      "28/28 [==============================] - 6s 219ms/step - loss: 0.0255 - accuracy: 0.9988\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - 6s 204ms/step - loss: 0.0245 - accuracy: 0.9992\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - 7s 243ms/step - loss: 0.0239 - accuracy: 0.9992\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 0.0220 - accuracy: 0.9990\n",
      "Epoch 75/100\n",
      "28/28 [==============================] - 6s 197ms/step - loss: 0.0200 - accuracy: 0.9991\n",
      "Epoch 76/100\n",
      "28/28 [==============================] - 6s 199ms/step - loss: 0.0214 - accuracy: 0.9995\n",
      "Epoch 77/100\n",
      "28/28 [==============================] - 6s 199ms/step - loss: 0.0193 - accuracy: 0.9999\n",
      "Epoch 78/100\n",
      "28/28 [==============================] - 6s 203ms/step - loss: 0.0182 - accuracy: 0.9977\n",
      "Epoch 79/100\n",
      "28/28 [==============================] - 6s 200ms/step - loss: 0.0175 - accuracy: 0.9999\n",
      "Epoch 80/100\n",
      "28/28 [==============================] - 6s 197ms/step - loss: 0.0170 - accuracy: 0.9996\n",
      "Epoch 81/100\n",
      "28/28 [==============================] - 6s 199ms/step - loss: 0.0164 - accuracy: 0.9997\n",
      "Epoch 82/100\n",
      "28/28 [==============================] - 6s 197ms/step - loss: 0.0137 - accuracy: 0.9997\n",
      "Epoch 83/100\n",
      "28/28 [==============================] - 7s 251ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "28/28 [==============================] - 5s 193ms/step - loss: 0.0128 - accuracy: 0.9998\n",
      "Epoch 85/100\n",
      "28/28 [==============================] - 6s 203ms/step - loss: 0.0123 - accuracy: 0.9999\n",
      "Epoch 86/100\n",
      "28/28 [==============================] - 6s 199ms/step - loss: 0.0125 - accuracy: 0.9998\n",
      "Epoch 87/100\n",
      "28/28 [==============================] - 6s 201ms/step - loss: 0.0124 - accuracy: 0.9997\n",
      "Epoch 88/100\n",
      "28/28 [==============================] - 6s 197ms/step - loss: 0.0115 - accuracy: 0.9998\n",
      "Epoch 89/100\n",
      "28/28 [==============================] - 5s 193ms/step - loss: 0.0108 - accuracy: 0.9999\n",
      "Epoch 90/100\n",
      "28/28 [==============================] - 6s 205ms/step - loss: 0.0119 - accuracy: 0.9996\n",
      "Epoch 91/100\n",
      "28/28 [==============================] - 6s 205ms/step - loss: 0.0094 - accuracy: 0.9997\n",
      "Epoch 92/100\n",
      "28/28 [==============================] - 6s 203ms/step - loss: 0.0091 - accuracy: 0.9999\n",
      "Epoch 93/100\n",
      "28/28 [==============================] - 6s 201ms/step - loss: 0.0096 - accuracy: 0.9999\n",
      "Epoch 94/100\n",
      "28/28 [==============================] - 6s 203ms/step - loss: 0.0094 - accuracy: 0.9996\n",
      "Epoch 95/100\n",
      "28/28 [==============================] - 6s 201ms/step - loss: 0.0087 - accuracy: 0.9996\n",
      "Epoch 96/100\n",
      "28/28 [==============================] - 6s 200ms/step - loss: 0.0076 - accuracy: 0.9997\n",
      "Epoch 97/100\n",
      "28/28 [==============================] - 6s 203ms/step - loss: 0.0076 - accuracy: 0.9998\n",
      "Epoch 98/100\n",
      "28/28 [==============================] - 7s 251ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "28/28 [==============================] - 6s 197ms/step - loss: 0.0074 - accuracy: 0.9998\n",
      "Epoch 100/100\n",
      "28/28 [==============================] - 6s 197ms/step - loss: 0.0074 - accuracy: 0.9997\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e93418ed90>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "model = Chatbot(vocab_size + 1)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([encoder_input, decoder_input], decoder_target, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(Path('model')/'model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(text: str):\n",
    "    text = text.lower()\n",
    "    text = unidecode(text)\n",
    "    tokenized_text = tokenizer.texts_to_sequences([text])\n",
    "    padded_text = pad_sequences(tokenized_text, maxlen=max_input_length, padding='post')\n",
    "\n",
    "    return padded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Chatbot(vocab_size + 1)\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.train_on_batch([encoder_input[:1], decoder_input[:1]], decoder_target[:1])\n",
    "#model.load_weights(Path('model')/'model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "la soga \n"
     ]
    }
   ],
   "source": [
    "question = 'Qué pelicula de Alfred Hitchcock se rodo como si fuera una toma continua'\n",
    "empty_target_seq = np.array([[tokenizer.word_index['start']]])\n",
    "stop_condition = False\n",
    "answer = ''\n",
    "stop_words = ['adios', 'gracias']\n",
    "\n",
    "proc_question = preprocess_input(question)\n",
    "\n",
    "h, c = model.encoder.predict(proc_question)\n",
    "\n",
    "while not stop_condition:\n",
    "    decoder_output, h, c = model.decoder.predict([empty_target_seq, h, c])\n",
    "    sampled_word_index = np.argmax( decoder_output[0, -1, :] )\n",
    "    sampled_word = tokenizer.index_word.get(sampled_word_index, None)\n",
    "    if sampled_word == 'end' or len(answer.split()) > max_output_length:\n",
    "        stop_condition = True\n",
    "    else:\n",
    "        answer += f\"{sampled_word} \"\n",
    "    \n",
    "\n",
    "    empty_target_seq = np.array([[sampled_word_index]])\n",
    "    enc_stats = [h, c]\n",
    "\n",
    "\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}